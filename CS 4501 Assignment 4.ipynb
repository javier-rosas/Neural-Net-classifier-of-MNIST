{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-40VPC7MAGGB"
   },
   "source": [
    "# Assignment 4: Benchmarking Fashion-MNIST with Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piFzh10hAGGE"
   },
   "source": [
    "### CS 4501 Machine Learning - Department of Computer Science - University of Virginia\n",
    "\"The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\" - **Zalando Research, Github Repo.**\"\n",
    "\n",
    "Fashion-MNIST is a dataset from the Zalando's article. Each example is a 28x28 grayscale image, associated with a label from 10 classes. They intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n",
    "\n",
    "![Here's an example how the data looks (each class takes three-rows):](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n",
    "\n",
    "In this assignment, you will attempt to benchmark the Fashion-MNIST using Neural Networks. You must use it to train some neural networks on TensorFlow and predict the final output of 10 classes. For deliverables, you must write code in Python and submit this Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "469YvvIzAGGJ"
   },
   "outputs": [],
   "source": [
    "# You might want to use the following packages\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) #reduce annoying warning messages\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-PtpH4xAGGG"
   },
   "source": [
    "---\n",
    "## 1. PRE-PROCESSING THE DATA (10 pts)\n",
    "\n",
    "You can load the Fashion MNIST directly from Tensorflow. **Partition of the dataset** so that you will have 50,000 examples for training, 10,000 examples for validation, and 10,000 examples for testing. Also, make sure that you platten out each of examples so that it contains only a 1-D feature vector.\n",
    "\n",
    "Write some code to output the dimensionalities of each partition (train, validation, and test sets).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2-Ilkesfm7Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (50000, 784)\n",
      "Shape of y_train: (50000,)\n",
      "Shape of x_validation: (10000, 784)\n",
      "Shape of y_validation: (10000,)\n",
      "Shape of x_test: (10000, 784)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here for this section\n",
    "fmnist = tf.keras.datasets.fashion_mnist.load_data();\n",
    "(x_train, y_train), (x_test, y_test) = fmnist[0], fmnist[1]\n",
    "\n",
    "\n",
    "# Flattening so that it contains only a 1-D feature vector:\n",
    "x_train = x_train.flatten().reshape( 60000, 28*28) \n",
    "x_test = x_test.flatten().reshape( 10000, 28*28) \n",
    "\n",
    "\n",
    "# Splitting x_train into 50,000 examples for training and 10,000 examples for validation: \n",
    "x_validation = x_train[0:10000]\n",
    "y_validation = y_train[0:10000]\n",
    "x_train = x_train[10000:60000]\n",
    "y_train = y_train[10000:60000]\n",
    "\n",
    "\n",
    "# Converting the x's to np.float32 and converting the y's to np.int32 for easier computations:\n",
    "x_train = x_train.astype( np.float32 )\n",
    "y_train = y_train.astype( np.int32 )\n",
    "x_validation = x_validation.astype( np.float32 )\n",
    "y_validation = y_validation.astype( np.int32 )\n",
    "x_test = x_test.astype( np.float32 )\n",
    "y_test = y_test.astype( np.int32 )\n",
    "\n",
    "\n",
    "# Using Pipeline with Standard Scaler to normalize/scale our data:\n",
    "pipeline = Pipeline([\n",
    "        ( 'std_scaler' , StandardScaler() ),\n",
    "    ])\n",
    "\n",
    "x_train = pipeline.fit_transform( x_train )\n",
    "x_validation = pipeline.fit_transform( x_validation )\n",
    "x_test = pipeline.fit_transform( x_test )\n",
    "\n",
    "\n",
    "# Printing shapes of x_train, y_train, x_validation, x_test and y_test:\n",
    "print( \"Shape of x_train:\", x_train.shape )\n",
    "print( \"Shape of y_train:\", y_train.shape )\n",
    "print( \"Shape of x_validation:\", x_validation.shape )\n",
    "print( \"Shape of y_validation:\", y_validation.shape )\n",
    "print( \"Shape of x_test:\", x_test.shape )\n",
    "print( \"Shape of y_test:\", y_test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Na4CpxLBAGGP"
   },
   "source": [
    "- - -\n",
    "## 2. CONSTRUCTION PHASE (30 pts)\n",
    "\n",
    "In this section, define at least three neural networks with different structures. Make sure that the input layer has the right number of inputs. The best structure often is found through a process of trial and error experimentation:\n",
    "- You may start with a fully connected network structure with two hidden layers.\n",
    "- You may try a few settings of the number of nodes in each layer.\n",
    "- You may try a few activation functions to see if they affect the performance.\n",
    "\n",
    "**Important Implementation Note:** For the purpose of learning Tensorflow, you must use low-level TensorFlow API to construct the network. Usage of high-level tools (ie. Keras) is not permited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DNN 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIJrHPVlAGGQ"
   },
   "outputs": [],
   "source": [
    "# First DNN uses a fully connected network structure with two hidden layers. \n",
    "# The first hidden layer has a dimensionality of 300, and the second hidden layer \n",
    "# has a dimensionality of 100. I am using the ReLu activation function. \n",
    "\n",
    "reset_graph() # need to reset the graph for neural network to work properly every time\n",
    "\n",
    "n_inputs = 28*28  # Fashion-MNIST\n",
    "learning_rate = 0.01\n",
    "n_hidden1 = 300 # dimensionality of first layer\n",
    "n_hidden2 = 100 # dimensionality of second layer \n",
    "n_outputs = 10 # 10 different integer classes (0 - 9)\n",
    "\n",
    "# Construct placeholder for the input layer\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDrFp7KKils6"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn1\"):\n",
    "    # Using 2 hidden layers, and activation function ReLu (tf.nn.relu):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    # Computes sparse softmax cross entropy between logits and labels:\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    # Optimizer implements gradient descent algorithm\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Minimizing the loss function \n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # Says whether the targets are in the top K predictions.\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    # Computes the mean of elements across dimensions of a tensor.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DNN 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luyNllyVkNxr"
   },
   "outputs": [],
   "source": [
    "# Second DNN uses a fully connected network structure with 3 hidden layers. \n",
    "# The first one with a dimensionality of 300, the second one with a dimensionality \n",
    "# of 100 and the third one with a dimensionality of 100. I am using the SELU activation \n",
    "# function. Also, I am implementing a dropout method for regularization. Using momentum \n",
    "# optimizer instead of regular gradient descent optimizer. \n",
    "\n",
    "reset_graph() # need to reset the graph for neural network to work properly every time\n",
    "\n",
    "\n",
    "n_inputs = 28*28  # Fashion-MNIST\n",
    "learning_rate = 0.01\n",
    "n_hidden1 = 300 # dimensionality of first layer \n",
    "n_hidden2 = 100 # dimensionality of second layer\n",
    "n_hidden3 = 100 # dimensionality of third layer\n",
    "\n",
    "n_outputs = 10 # 10 different integer classes (0 - 9)\n",
    "\n",
    "# Construct placeholder for the input layer\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn2\"):\n",
    "    # Using 3 hidden layers, and activation function SELU (tf.nn.selu) and dropping\n",
    "    # at a rate of 0.5 \n",
    "\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.selu,name=\"hidden3\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.selu, name=\"hidden4\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, activation=tf.nn.selu, name=\"hidden5\")\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "\n",
    "    logits = tf.layers.dense(hidden3_drop, n_outputs, name=\"outputs1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    # Computes sparse softmax cross entropy between logits and labels:\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    # Optimizer implements the momentum algorithm\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    # Minimizing the loss function \n",
    "    training_op = optimizer.minimize(loss)       \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # Says whether the targets are in the top K predictions.\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    # Computes the mean of elements across dimensions of a tensor.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DNN 3:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JuvjZeJekP4-"
   },
   "outputs": [],
   "source": [
    "# Third DNN uses a fully connected network structure with 5 hidden layers. \n",
    "# The first one with a dimensionality of 300, the second one with a dimensionality \n",
    "# of 100, the third one with a dimensionality of 50, the fourth one with a \n",
    "# dimensionality of 50 and the fifth one with a dimensionality of 50. \n",
    "# I am using the SELU activation function. I am also training with Gradient Clipping, \n",
    "# because large updates to weights during training can cause a numerical overflow or underflow.\n",
    "\n",
    "\n",
    "reset_graph() # need to reset the graph for neural network to work properly every time\n",
    "\n",
    "\n",
    "n_inputs = 28*28  # Fashion-MNIST\n",
    "learning_rate = 0.01\n",
    "n_hidden1 = 300 # dimensionality of first layer \n",
    "n_hidden2 = 100 # dimensionality of second layer \n",
    "n_hidden3 = 50 # dimensionality of third layer \n",
    "n_hidden4 = 50 # dimensionality of fourth layer \n",
    "n_hidden5 = 50 # dimensionality of fifth layer \n",
    "n_outputs = 10\n",
    "\n",
    "# Construct placeholder for the input layer\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn3\"):\n",
    "    # Using 5 layers with activation function Elu\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden6\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name=\"hidden7\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.elu, name=\"hidden8\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.elu, name=\"hidden9\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.elu, name=\"hidden10\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mQCboA8ijWK"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL_cXX09ih12"
   },
   "outputs": [],
   "source": [
    "# Training with Gradient Clipping so that gradients don't \"explode\". First we clip by \n",
    "# using the clip_by_value() function. Then we apply the clip with the threshold. \n",
    "\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfKWL5IZigJg"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # Says whether the targets are in the top K predictions.\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    # Computes the mean of elements across dimensions of a tensor.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKcVSGXOAGGT"
   },
   "source": [
    "- - -\n",
    "## 3. EXECUTION PHASE (30 pts)\n",
    "\n",
    "After you construct the three models of neural networks, you can compute the performance measure as the class accuracy. You will need to define the number of epochs and size of the training batch. You also might need to reset the graph each time your try a different model. To save time and avoid retraining, you should save the trained model and load it from disk to evaluate a test set. Pick the best model and answer the following:\n",
    "- Which model yields the best performance measure for your dataset? Provide a reason why it yields the best performance.\n",
    "- Why did you pick this many hidden layers?\n",
    "- Provide some justifiable reasons for selecting the number of neurons per hidden layers. \n",
    "- Which activation functions did you use?\n",
    "\n",
    "In the next session you will get a chance to finetune it further .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGDKdeZzAGGV"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "# shuffle_batch() shuffle the examples in a batch before training\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKMqf1gijPwW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8576\n",
      "1 Validation accuracy: 0.8584\n",
      "2 Validation accuracy: 0.8669\n",
      "3 Validation accuracy: 0.875\n",
      "4 Validation accuracy: 0.8823\n",
      "5 Validation accuracy: 0.8776\n",
      "6 Validation accuracy: 0.8843\n",
      "7 Validation accuracy: 0.8795\n",
      "8 Validation accuracy: 0.8845\n",
      "9 Validation accuracy: 0.8854\n",
      "10 Validation accuracy: 0.8885\n",
      "11 Validation accuracy: 0.889\n",
      "12 Validation accuracy: 0.8899\n",
      "13 Validation accuracy: 0.887\n",
      "14 Validation accuracy: 0.8816\n",
      "15 Validation accuracy: 0.8927\n",
      "16 Validation accuracy: 0.8847\n",
      "17 Validation accuracy: 0.8882\n",
      "18 Validation accuracy: 0.8839\n",
      "19 Validation accuracy: 0.8934\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for X_batch, y_batch in shuffle_batch(x_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: x_validation, y: y_validation})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)        \n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_dnn_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFPM62nDjncP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 88.80%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_dnn_model.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: x_test, y: y_test})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Answers to questions:**\n",
    "- Which model yields the best performance measure for your dataset? Provide a reason why it yields the best performance. \n",
    "    \n",
    "The model that yielda the best accuracy is DNN # 2, because it has a final test accuracy of 88%, which is only abou t 1% away from the benchmark (89.7%). This was a fairly simple model composed of 3 hidden layers. The first, second and third layers each had 300 neurons, 100 neurons and 100 neurons, respectively. In my opinion, the model yields the best performance because it is not to complex (only using 3 hidden layers), and I implemented the dropout regularization technique in order to avoid overfitting on my DNN. \n",
    "\n",
    "    \n",
    "    \n",
    "- Why did you pick this many hidden layers?\n",
    "\n",
    "I chose only 3 hidden layers because I wanted to keep my model simple, easy to implement and computationally convenient. With too many hidden layers, the computational complexity increases drastically and the time that it takes to train the model also increases greatly.\n",
    "\n",
    "    \n",
    "    \n",
    "- Provide some justifiable reasons for selecting the number of neurons per hidden layers. \n",
    "\n",
    "For my first, second and third layers, I chose 300 neurons, 100 neurons and 100 neurons respectively. I chose these many neurons per layer because I wanted to reduce the number of neurons after the first layer by 1/3 to simplify the model and make it more computationally feasable to compute. I also did some trial and error, and found that 100 neurons on the last layer resulted in better final test accuracies, so I decided to stick with it. \n",
    "\n",
    "    \n",
    "\n",
    "- Which activation functions did you use?\n",
    "\n",
    "I used the SElU activation function for every one of my layers, which seemed to work pretty well. After trying ReLu on every layer, I found that the final test accuracies were much better with the SELU activation function. This might be due to SELU's slightly better performance when determining the activation node probabilities, when compared to ReLu.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-s2zv1SrAGGY"
   },
   "source": [
    "- - -\n",
    "## 4. FINETUNING THE NETWORK (25 pts)\n",
    "\n",
    "The best performance on the Fashion MNIST of a non-neural-net classifier is the Support Vector Classifier {\"C\":10,\"kernel\":\"poly\"} with 0.897 accuracy. In this section, you will see how close you can get to that accuracy, or (better yet) beat it! You will be able to see the performance of other ML methods below:\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com\n",
    "\n",
    "Use the best model from the previous section and see if you can improve it further. To improve the performance of your model, You must make some modifications based upon the practical guidelines discuss in class. Here are a few decisions about the recommended network configurations you have to make:\n",
    "1. Initialization: Use He Initialization for your model\n",
    "2. Activation: Add ELU as the activation function throughout your hidden layers\n",
    "3. Normalization: Incorporate the batch normalization at every layer\n",
    "4. Regularization: Configure the dropout policy at 50% rate\n",
    "5. Optimization: Change Gradient Descent into Adam Optimization\n",
    "6. Your choice: make any other changes in 1-5 you deem necessary\n",
    "\n",
    "Keep in mind that the execution phase is essentially the same, so you can just run it from the above. See how much you gain in classification accuracy. Provide some justifications for the gain in performance. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finetuned Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjDXZ5ws6Zpx"
   },
   "outputs": [],
   "source": [
    "reset_graph() # need to reset the graph for neural network to work properly every time\n",
    "\n",
    "\n",
    "n_inputs = 28*28  # Fashion-MNIST\n",
    "learning_rate = 0.01\n",
    "n_hidden1 = 300 # dimensionality of first layer \n",
    "n_hidden2 = 100 # dimensionality of second layer\n",
    "n_hidden3 = 100 # dimensionality of third layer\n",
    "\n",
    "n_outputs = 10 # 10 different integer classes (0 - 9)\n",
    "\n",
    "# Momentum for the batch normalization \n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "# Construct placeholder for the input layer\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# Dropping at a rate of 0.5\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnnBenchmark\"):\n",
    "    # Using 3 hidden layers, and activation function ELU (tf.nn.elu) and dropping\n",
    "    # at a rate of 0.5. Also using kernel_initializer=he_init for He initialization. \n",
    "    # We are also using Adam Optimizer instead of Gradient Descent. \n",
    "    \n",
    "    # He Initialization\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    # To avoid repeating the same parameters over and over again, we can use Python's partial() function\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization, # Using batch normalization \n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X_drop, n_hidden1, name=\"hidden11\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training) # Dropping at a rate of 0.5\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1_drop)) # Using elu as an activation function \n",
    "    \n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden12\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training) # Dropping at a rate of 0.5\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2_drop)) # Using elu as an activation function \n",
    "    \n",
    "    hidden3 = my_dense_layer(bn2, n_hidden3, name=\"hidden13\")\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training) # Dropping at a rate of 0.5\n",
    "    bn3 = tf.nn.elu(my_batch_norm_layer(hidden3_drop)) # Using elu as an activation function \n",
    "    \n",
    "    \n",
    "    logits_before_bn = my_dense_layer(bn3, n_outputs, name=\"outputs3\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    # Computes sparse softmax cross entropy between logits and labels:\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    # Optimizer implements the momentum algorithm\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Using Adam Optimizer\n",
    "    # Minimizing the loss function \n",
    "    training_op = optimizer.minimize(loss)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # Says whether the targets are in the top K predictions.\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    # Computes the mean of elements across dimensions of a tensor.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04jsbI9TAGGY"
   },
   "source": [
    "- - -\n",
    "## 5. OUTLOOK (5 pts)\n",
    "\n",
    "Plan for the outlook of your system: This may lead to the direction of your future project:\n",
    "- Did your neural network outperform other \"traditional ML technique? Why/why not?\n",
    "\n",
    "The \"traditional ML technique\" benchmark had an accuracy of 89.7%. After all of the finetuning, I was able to increase the accuracy of my DNN to 88.8%. This is about 1% away from the benchmark, which in my opinion is not so bad. I think I did not outperform the benchmark because I did not have enough time to try all the different combinations of hyperparameters like drop out rates, momentum rates, etc. I also only used 3 hidden layers, which might have affected the accuracy of the model. My intention was to keep the model simple, but maybe if I tried a more complex model with more layers and different combinations of neurons in each layer, I could have improved the performance of the model.\n",
    "- Does your model work well? If not, which model should be further investigated?\n",
    "\n",
    "The finetuned model seems to work quite well. In fact, all of the 4 DNNs that I created had accuracies of over 86%. The worst model, which had an accuracy of 86% was using the ReLu activation function. So I suspect thats why it performed so poorly compared to the others. \n",
    "\n",
    "- Do you satisfy with your system? What do you think needed to improve?\n",
    "\n",
    "I am certainly satisfied with my system but I think that if I had more time, I could perform a trial and error process to find the best possible DNN with the right number of layers and right number of nodes for each layer. I could also do a trial and error process in order to find the best hyper parameters and activation functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zS9PKaL4AGGZ"
   },
   "source": [
    "- - - \n",
    "### NEED HELP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0vuIEBDAGGa"
   },
   "source": [
    "In case you get stuck in any step in the process, you may find some useful information from:\n",
    "\n",
    " * Consult my lectures and/or the textbook\n",
    " * Talk to the TA, they are available and there to help you during OH\n",
    " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4501 Assignment 4:...\".\n",
    " * More on the Fashion-MNIST to be found here: https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/\n",
    "\n",
    "Best of luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cH_mulWEAGGb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 4501 Assignment 4.ipynb",
   "provenance": [
    {
     "file_id": "1hQZ4t2l5aFDO0sEs213HsV547c_tH6TL",
     "timestamp": 1554445243544
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
